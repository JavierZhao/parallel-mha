{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0b10a24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T16:24:46.312937Z",
     "start_time": "2024-05-20T16:24:46.303274Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4b8ac10a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T17:08:38.721186Z",
     "start_time": "2024-05-20T17:08:38.663439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights set\n",
      "torch.Size([2, 4])\n",
      "q: tensor([[[ 0.2494,  1.5786],\n",
      "         [-0.2285,  0.1889]],\n",
      "\n",
      "        [[-0.0634, -0.2776],\n",
      "         [ 0.0428,  1.1328]]])\n",
      "k: tensor([[[-0.0716, -0.3137],\n",
      "         [ 1.5609, -0.7852]],\n",
      "\n",
      "        [[-0.0297,  0.6089],\n",
      "         [ 0.0216,  1.0270]]])\n",
      "v: tensor([[[-0.0751,  2.2267],\n",
      "         [-0.7416, -0.0169]],\n",
      "\n",
      "        [[-1.1777, -1.1061],\n",
      "         [ 0.1794, -1.1788]]])\n",
      "sqrt(dk): tensor([1.4142])\n",
      "q * k / sqrt(dk): tensor([[[-0.3628, -0.6012],\n",
      "         [-0.0303, -0.3571]],\n",
      "\n",
      "        [[-0.1182, -0.2026],\n",
      "         [ 0.4868,  0.8233]]])\n",
      "softmax: tensor([[[0.5593, 0.4407],\n",
      "         [0.5810, 0.4190]],\n",
      "\n",
      "        [[0.5211, 0.4789],\n",
      "         [0.4167, 0.5833]]])\n",
      "softmax * v: tensor([[[-0.3688,  1.2380],\n",
      "         [-0.3544,  1.2866]],\n",
      "\n",
      "        [[-0.5278, -1.1409],\n",
      "         [-0.3860, -1.1485]]])\n",
      "softmax * v shape: torch.Size([2, 4])\n",
      "Output Tensor:\n",
      " tensor([[-0.6420, -0.3552,  0.4712, -0.0707],\n",
      "        [ 0.2790, -0.2336, -0.0741,  0.0035]])\n",
      "Output Tensor:\n",
      " torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Load a matrix from a CSV file\n",
    "def load_csv_as_tensor(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    matrix = torch.tensor(df.values, dtype=torch.float32)\n",
    "    return matrix\n",
    "\n",
    "# class CustomMultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "#     def set_custom_weights(self, Wqkv, Wo, bqkv, bo):\n",
    "#         d_k = Wqkv.size(1) // 3\n",
    "#         self.mha.in_proj_weight.data = Wqkv\n",
    "#         self.mha.in_proj_bias.data = bqkv\n",
    "#         self.mha.out_proj.weight.data = Wo\n",
    "#         self.mha.out_proj.bias.data = bo\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output, attn_weights = self.mha(x, x, x)\n",
    "#         return output\n",
    "\n",
    "class CustomMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, Wqkv=None, Wo=None, bqkv=None, bo=None):\n",
    "        super(CustomMultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Define the projections for queries, keys, and values\n",
    "        self.proj_q = torch.zeros((embed_dim, embed_dim))\n",
    "        self.proj_k = torch.zeros((embed_dim, embed_dim))\n",
    "        self.proj_v = torch.zeros((embed_dim, embed_dim))\n",
    "\n",
    "        # Define the output projection\n",
    "        self.proj_out = torch.zeros((embed_dim, embed_dim))\n",
    "\n",
    "        # Scaling factor to prevent the softmax from having too large/small gradients\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]))\n",
    "        \n",
    "        # Optionally set weights and biases\n",
    "        if Wqkv is not None:\n",
    "            # Set weights for Q, K, V projections\n",
    "            self.proj_q = Wqkv[:embed_dim]\n",
    "            self.proj_k = Wqkv[embed_dim:2*embed_dim]\n",
    "            self.proj_v = Wqkv[2*embed_dim:3*embed_dim]\n",
    "            print(\"weights set\")\n",
    "#             if bqkv is not None:\n",
    "#                 # Set biases for Q, K, V projections\n",
    "#                 self.proj_q.bias.data = bqkv[:embed_dim]\n",
    "#                 self.proj_k.bias.data = bqkv[embed_dim:2*embed_dim]\n",
    "#                 self.proj_v.bias.data = bqkv[2*embed_dim:3*embed_dim]\n",
    "        if Wo is not None and bo is not None:\n",
    "            # Set weights and biases for output projection\n",
    "            self.proj_out = Wo\n",
    "#             self.proj_out.bias.data = bo\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length, embed_dim = x.size()\n",
    "\n",
    "        # Project the queries, keys, and values\n",
    "        q = (x @ self.proj_q).view(seq_length, self.num_heads, self.head_dim)\n",
    "        print(f\"q: {q}\")\n",
    "        k = (x @ self.proj_k).view(seq_length, self.num_heads, self.head_dim)\n",
    "        print(f\"k: {k}\")\n",
    "        v = (x @ self.proj_v).view(seq_length, self.num_heads, self.head_dim)\n",
    "        print(f\"v: {v}\")\n",
    "\n",
    "        # Transpose for attention computation: b x n x l x d -> b x l x n x d\n",
    "#         q = q.transpose(1, 2)\n",
    "#         k = k.transpose(1, 2)\n",
    "#         v = v.transpose(1, 2)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        scores = torch.matmul(q / self.scale, k.transpose(-2, -1))\n",
    "        print(f\"sqrt(dk): {self.scale}\")\n",
    "        print(f\"q * k / sqrt(dk): {scores}\")\n",
    "        attn = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        print(f\"softmax: {attn}\")\n",
    "\n",
    "        # Apply attention to the values\n",
    "        context = torch.matmul(attn, v)\n",
    "        print(f\"softmax * v: {context}\")\n",
    "\n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(seq_length, embed_dim)\n",
    "        print(f\"softmax * v shape: {context.shape}\")\n",
    "\n",
    "        # Final output projection\n",
    "        output = context @ self.proj_out\n",
    "        return output, attn\n",
    "\n",
    "# Parameters\n",
    "embed_dim = 4\n",
    "num_heads = 2\n",
    "\n",
    "# Load weights and biases from CSV\n",
    "base_dir = \"/Users/zhaozihan/Desktop/PHYS 244/parallel-mha/serial_code/compare_python\"\n",
    "\n",
    "Wq = load_csv_as_tensor(f'{base_dir}/Wq.csv')\n",
    "Wk = load_csv_as_tensor(f'{base_dir}/Wk.csv')\n",
    "Wv = load_csv_as_tensor(f'{base_dir}/Wv.csv')\n",
    "Wo = load_csv_as_tensor(f'{base_dir}/Wo.csv')\n",
    "bq = load_csv_as_tensor(f'{base_dir}/bq.csv')\n",
    "bk = load_csv_as_tensor(f'{base_dir}/bk.csv')\n",
    "bv = load_csv_as_tensor(f'{base_dir}/bv.csv')\n",
    "bo = load_csv_as_tensor(f'{base_dir}/bo.csv')\n",
    "\n",
    "# Concatenate Wq, Wk, Wv into a single weight matrix for in_proj\n",
    "Wqkv = torch.cat((Wq, Wk, Wv), dim=0)\n",
    "bqkv = torch.cat((bq, bk, bv), dim=0)\n",
    "\n",
    "# Initialize the model\n",
    "mha = CustomMultiHeadAttention(embed_dim, num_heads, Wqkv=Wqkv, Wo=Wo, bqkv=bqkv, bo=bo)\n",
    "\n",
    "# Load input\n",
    "x = load_csv_as_tensor('mha_input.csv')  # Add batch dimension\n",
    "print(x.shape)\n",
    "\n",
    "# Compute output\n",
    "output = mha(x)\n",
    "\n",
    "# Print output\n",
    "print(\"Output Tensor:\\n\", output[0])\n",
    "print(\"Output Tensor:\\n\", output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1ba5d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T15:36:02.586652Z",
     "start_time": "2024-05-20T15:36:02.570386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1277, -0.5129,  0.4347, -0.2219],\n",
       "        [ 0.3021, -1.0609,  1.0369, -0.8575]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_out = load_csv_as_tensor('mha_output.csv')\n",
    "expected_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
