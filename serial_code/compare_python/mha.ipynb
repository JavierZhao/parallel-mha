{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b10a24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T16:24:46.312937Z",
     "start_time": "2024-05-20T16:24:46.303274Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8ac10a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T17:08:38.721186Z",
     "start_time": "2024-05-20T17:08:38.663439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wq tensor([[-0.3575,  0.5264,  0.2733,  0.2968],\n",
      "        [ 0.0322, -0.3140,  0.1036, -0.5718],\n",
      "        [-0.4425, -0.3157,  0.3864,  0.3590],\n",
      "        [-0.2716, -0.0221,  0.3916,  0.6088]])\n",
      "Wq size torch.Size([4, 4])\n",
      "embed_dim 4\n",
      "num_heads 2\n",
      "head_dim 2\n",
      "weights set\n",
      "torch.Size([2, 4])\n",
      "q: tensor([[[-1.1320, -0.4090],\n",
      "         [-1.2269, -0.9193]],\n",
      "\n",
      "        [[ 0.0104, -0.0332],\n",
      "         [ 1.3781,  0.9936]]])\n",
      "k: tensor([[[-0.7493,  1.0601],\n",
      "         [ 1.2502,  0.5704]],\n",
      "\n",
      "        [[-0.5158,  0.1040],\n",
      "         [-0.6362, -0.1282]]])\n",
      "v: tensor([[[ 1.3135, -0.6319],\n",
      "         [ 0.0035, -2.4008]],\n",
      "\n",
      "        [[-1.6128, -0.3029],\n",
      "         [ 0.3143, -0.4661]]])\n",
      "sqrt(dk): tensor([1.4142])\n",
      "q * k / sqrt(dk): tensor([[[ 0.2932,  0.3828],\n",
      "         [-0.0304, -0.0062]],\n",
      "\n",
      "        [[-1.4555,  0.6353],\n",
      "         [ 1.6191, -0.7100]]])\n",
      "softmax: tensor([[[0.4776, 0.5224],\n",
      "         [0.4940, 0.5060]],\n",
      "\n",
      "        [[0.1100, 0.8900],\n",
      "         [0.9113, 0.0887]]])\n",
      "softmax * v: tensor([[[-0.2152, -0.4600],\n",
      "         [-0.1673, -0.4654]],\n",
      "\n",
      "        [[ 0.2801, -0.6789],\n",
      "         [ 0.0311, -2.2291]]])\n",
      "softmax * v shape: torch.Size([2, 4])\n",
      "Output Tensor:\n",
      " tensor([[-0.1277, -0.5129,  0.4347, -0.2219],\n",
      "        [ 0.3021, -1.0609,  1.0369, -0.8575]])\n",
      "Output Tensor:\n",
      " torch.Size([2, 4])\n",
      "Output Attention:\n",
      " tensor([[[0.4776, 0.5224],\n",
      "         [0.4940, 0.5060]],\n",
      "\n",
      "        [[0.1100, 0.8900],\n",
      "         [0.9113, 0.0887]]])\n"
     ]
    }
   ],
   "source": [
    "# Load a matrix from a CSV file\n",
    "def load_csv_as_tensor(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    matrix = torch.tensor(df.values, dtype=torch.float32)\n",
    "    return matrix\n",
    "\n",
    "# class CustomMultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "#     def set_custom_weights(self, Wqkv, Wo, bqkv, bo):\n",
    "#         d_k = Wqkv.size(1) // 3\n",
    "#         self.mha.in_proj_weight.data = Wqkv\n",
    "#         self.mha.in_proj_bias.data = bqkv\n",
    "#         self.mha.out_proj.weight.data = Wo\n",
    "#         self.mha.out_proj.bias.data = bo\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output, attn_weights = self.mha(x, x, x)\n",
    "#         return output\n",
    "\n",
    "class CustomMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, Wqkv=None, Wo=None, bqkv=None, bo=None):\n",
    "        super(CustomMultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        print(\"embed_dim\", embed_dim)\n",
    "        print(\"num_heads\", self.num_heads)\n",
    "        print(\"head_dim\", self.head_dim)\n",
    "\n",
    "        # Define the projections for queries, keys, and values\n",
    "        self.proj_q = torch.zeros((embed_dim, embed_dim))\n",
    "        self.proj_k = torch.zeros((embed_dim, embed_dim))\n",
    "        self.proj_v = torch.zeros((embed_dim, embed_dim))\n",
    "\n",
    "        # Define the output projection\n",
    "        self.proj_out = torch.zeros((embed_dim, embed_dim))\n",
    "\n",
    "        # Scaling factor to prevent the softmax from having too large/small gradients\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]))\n",
    "        \n",
    "        # Optionally set weights and biases\n",
    "        if Wqkv is not None:\n",
    "            # Set weights for Q, K, V projections\n",
    "            self.proj_q = Wqkv[:embed_dim]\n",
    "            self.proj_k = Wqkv[embed_dim:2*embed_dim]\n",
    "            self.proj_v = Wqkv[2*embed_dim:3*embed_dim]\n",
    "            print(\"weights set\")\n",
    "#             if bqkv is not None:\n",
    "#                 # Set biases for Q, K, V projections\n",
    "#                 self.proj_q.bias.data = bqkv[:embed_dim]\n",
    "#                 self.proj_k.bias.data = bqkv[embed_dim:2*embed_dim]\n",
    "#                 self.proj_v.bias.data = bqkv[2*embed_dim:3*embed_dim]\n",
    "        if Wo is not None and bo is not None:\n",
    "            # Set weights and biases for output projection\n",
    "            self.proj_out = Wo\n",
    "#             self.proj_out.bias.data = bo\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length, embed_dim = x.size()\n",
    "\n",
    "        # Project the queries, keys, and values\n",
    "        # x: (seq_length, embed_dim)\n",
    "        # proj: (embeded_dim, num_heads * head_dim)\n",
    "        # (x @ proj): (seq_length, num_heads * head_dim)\n",
    "        # q, k, v: (seq_length, num_heads, head_dim)\n",
    "        q = (x @ self.proj_q.transpose(0, 1)).view(seq_length, self.num_heads, self.head_dim)\n",
    "        print(f\"q: {q}\")\n",
    "        k = (x @ self.proj_k.transpose(0, 1)).view(seq_length, self.num_heads, self.head_dim)\n",
    "        print(f\"k: {k}\")\n",
    "        v = (x @ self.proj_v.transpose(0, 1)).view(seq_length, self.num_heads, self.head_dim)\n",
    "        print(f\"v: {v}\")\n",
    "\n",
    "        # Transpose for attention computation: b x n x l x d -> b x l x n x d\n",
    "        # q, k, v: (num_heads, seq_length, head_dim)\n",
    "        q = q.transpose(0, 1)\n",
    "        k = k.transpose(0, 1)\n",
    "        v = v.transpose(0, 1)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        # scores: (num_heads, seq_length, seq_length)\n",
    "        scores = torch.matmul(q / self.scale, k.transpose(-2, -1))\n",
    "        print(f\"sqrt(dk): {self.scale}\")\n",
    "        print(f\"q * k / sqrt(dk): {scores}\")\n",
    "        attn = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        print(f\"softmax: {attn}\")\n",
    "\n",
    "        # Apply attention to the values\n",
    "        # context: (num_heads, seq_length, head_dim)\n",
    "        context = torch.matmul(attn, v)\n",
    "        print(f\"softmax * v: {context}\")\n",
    "\n",
    "        # Concatenate heads\n",
    "        # contect: (seq_length, embed_dim)\n",
    "        context = context.transpose(0, 1).contiguous().view(seq_length, embed_dim)\n",
    "        print(f\"softmax * v shape: {context.shape}\")\n",
    "\n",
    "        # Final output projection\n",
    "        output = context @ self.proj_out.transpose(0, 1)\n",
    "        return output, attn\n",
    "\n",
    "# Parameters\n",
    "embed_dim = 4\n",
    "num_heads = 2\n",
    "\n",
    "# Load weights and biases from CSV\n",
    "base_dir = \"/Users/billyli/Documents/UCSD course files/PHYS 244/project/parallel-mha/serial_code/compare_python\"\n",
    "\n",
    "# comopare cpp: Wq d_model, d_model\n",
    "# in set_weights, it is interpreted (d_model, num_heads*depth)\n",
    "Wq = load_csv_as_tensor(f'{base_dir}/Wq.csv')\n",
    "Wk = load_csv_as_tensor(f'{base_dir}/Wk.csv')\n",
    "Wv = load_csv_as_tensor(f'{base_dir}/Wv.csv')\n",
    "Wo = load_csv_as_tensor(f'{base_dir}/Wo.csv')\n",
    "bq = load_csv_as_tensor(f'{base_dir}/bq.csv')\n",
    "bk = load_csv_as_tensor(f'{base_dir}/bk.csv')\n",
    "bv = load_csv_as_tensor(f'{base_dir}/bv.csv')\n",
    "bo = load_csv_as_tensor(f'{base_dir}/bo.csv')\n",
    "\n",
    "print(\"Wq\", Wq)\n",
    "print(\"Wq size\", Wq.size())\n",
    "\n",
    "# Concatenate Wq, Wk, Wv into a single weight matrix for in_proj\n",
    "Wqkv = torch.cat((Wq, Wk, Wv), dim=0)\n",
    "bqkv = torch.cat((bq, bk, bv), dim=0)\n",
    "\n",
    "# Initialize the model\n",
    "mha = CustomMultiHeadAttention(embed_dim, num_heads, Wqkv=Wqkv, Wo=Wo, bqkv=bqkv, bo=bo)\n",
    "\n",
    "# Load input\n",
    "x = load_csv_as_tensor('mha_input.csv')  # Add batch dimension\n",
    "print(x.shape)\n",
    "\n",
    "# Compute output\n",
    "output = mha(x)\n",
    "\n",
    "# Print output\n",
    "print(\"Output Tensor:\\n\", output[0])\n",
    "print(\"Output Tensor:\\n\", output[0].shape)\n",
    "\n",
    "print(\"Output Attention:\\n\", output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ba5d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T15:36:02.586652Z",
     "start_time": "2024-05-20T15:36:02.570386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1277, -0.5129,  0.4347, -0.2219],\n",
       "        [ 0.3021, -1.0609,  1.0369, -0.8575]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_out = load_csv_as_tensor('mha_output.csv')\n",
    "expected_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93301e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
